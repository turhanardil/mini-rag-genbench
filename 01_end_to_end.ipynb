{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Generative Benchmark: End-to-End Notebook\n",
    "\n",
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q datasets pyarrow tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## 2. Load Data\n",
    "We’ll fetch **500 articles** from the AG News corpus.\n",
    "Each record is already a clean news snippet, which is ideal for a lightweight benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from datasets import load_dataset \n",
    "\n",
    "RAW_DIR = Path(\"data\", \"raw\")\n",
    "PROC_DIR = Path(\"data\", \"processed\")\n",
    "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PROC_DIR.mkdir(parents=True,exist_ok=True)\n",
    "\n",
    "SAMPLE_SIZE = 500\n",
    "\n",
    "ag_ds = load_dataset(\n",
    "    \"ag_news\",\n",
    "    split=f\"train[:{SAMPLE_SIZE}]\",\n",
    "    cache_dir=str(RAW_DIR)\n",
    ")\n",
    "\n",
    "print(f\"fetched {len(ag_ds):,} articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_doc(row, idx: int) -> dict:\n",
    "    \"\"\"\n",
    "    Convert a HuggingFace row into {'doc_id', 'content'}.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"doc_id\": f\"ag_{idx:04d}\",\n",
    "        \"content\": row[\"text\"].strip()\n",
    "    }\n",
    "\n",
    "docs = [to_doc(ag_ds[i], i) for i in range(len(ag_ds))]\n",
    "print(docs[0][\"doc_id\"], \"->\", docs[0][\"content\"][:80], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tiktoken\n",
    "\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "lengths = [len(enc.encode(d[\"content\"])) for d in docs]\n",
    "\n",
    "print(\n",
    "    f\"avg tokens: {np.mean(lengths):.1f} | \"\n",
    "    f\"min: {np.min(lengths)} | \"\n",
    "    f\"max: {np.max(lengths)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "out_path = Path(\"data\", \"processed\", \"docs.parquet\")\n",
    "pd.DataFrame(docs).to_parquet(out_path, index=False)\n",
    "print(\"saved -> \", out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## 3. Chunk / Embed / Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q sentence-transformers chromadb langchain langchain-community tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U langchain-huggingface langchain-chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from src.chunker import chunk_documents\n",
    "\n",
    "docs = pd.read_parquet(\"data/processed/docs.parquet\").to_dict(orient=\"records\")\n",
    "chunks = chunk_documents(docs, chunk_size=400, chunk_overlap=50)\n",
    "\n",
    "print(f\"{len(docs)} docs -> {len(chunks)} chunks\")\n",
    "print(\"example chunk id :\", chunks[0][\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.indexer import embed_and_index\n",
    "\n",
    "retriever_vs = embed_and_index(\n",
    "    chunks,\n",
    "    collection_name=\"ag_miniLM\",\n",
    "    persist_path=\"data/chroma\",\n",
    ")\n",
    "\n",
    "print(\"collection size:\", retriever_vs._collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q huggingface_hub jsonlines tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### 4 Synthetic query generation (fast)\n",
    "\n",
    "We’ll use **google/flan-t5-small** locally, in batches of 16, to produce\n",
    "one question per chunk in just a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q transformers accelerate jsonlines tqdm\n",
    "\n",
    "import textwrap, pathlib, jsonlines, tqdm\n",
    "from transformers import pipeline\n",
    "from src.chunker import chunk_documents\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = pd.read_parquet(\"data/processed/docs.parquet\").to_dict(\"records\")\n",
    "chunks = chunk_documents(docs, chunk_size=400, chunk_overlap=50)\n",
    "\n",
    "generator = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=\"google/flan-t5-small\",\n",
    "    device=-1, #force cpu\n",
    ")\n",
    "\n",
    "# plain T5 format\n",
    "PROMPT = \"generate question: \\\"{passage}\\\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = pathlib.Path(\"data/queries.jsonl\")\n",
    "batch_size = 16\n",
    "\n",
    "if out_path.exists():\n",
    "    print(\"Cached queries found:\", out_path)\n",
    "else:\n",
    "    with jsonlines.open(out_path, \"w\") as writer:\n",
    "        for i in tqdm.trange(0, len(chunks), batch_size, desc=\"batches\"):\n",
    "            batch = chunks[i : i + batch_size]\n",
    "            prompts = [PROMPT.format(passage=c[\"text\"].strip()) for c in batch]\n",
    "            #run 16 passages in one go\n",
    "            outputs = generator(prompts, max_new_tokens=48)\n",
    "            for c, out in zip(batch, outputs):\n",
    "                q = out[\"generated_text\"].strip()\n",
    "                writer.write(\n",
    "                    {\"query\": q, \"chunk_id\": c[\"id\"], \"parent_id\": c[\"parent_id\"]}\n",
    "                )\n",
    "    print(\"Saved\", len(chunks), \"queries to\", out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, jsonlines\n",
    "rows = list(jsonlines.open(out_path))\n",
    "print(\"Total queries:\", len(rows))\n",
    "for r in random.sample(rows, min(5, len(rows))):\n",
    "    print(f\"[{r['chunk_id']}] {r['query']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "### 5. Evaluation (Recall@k)\n",
    "\n",
    "We now measure how often the correct chunk appears in the top k results for each synthetic query.  \n",
    "We report Recall@1, @3 and @5, then save a bar chart under `figures/recall_baseline.png`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma as LCChroma\n",
    "\n",
    "#load golden queries\n",
    "queries = list(jsonlines.open(\"data/queries.jsonl\"))\n",
    "print(f\"Loaded {len(queries)} synthetic queries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "assert \"queries\" in globals() and \"retriever_vs\" in globals(), \\\n",
    "    \"You need to run Section 3 (chunking & indexing) first\"\n",
    "\n",
    "def recall_at_k(queries, retriever, k):\n",
    "    hits = 0\n",
    "    for entry in queries:\n",
    "        docs = retriever.invoke(entry[\"query\"])   \n",
    "        top_parents = [doc.metadata[\"parent_id\"] for doc in docs]\n",
    "        if entry[\"parent_id\"] in top_parents:\n",
    "            hits += 1\n",
    "    return hits / len(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = [1, 3, 5]\n",
    "results = {}\n",
    "\n",
    "for k in ks:\n",
    "    retriever = retriever_vs.as_retriever(search_kwargs={\"k\": k})\n",
    "    score = recall_at_k(queries, retriever, k)\n",
    "    results[k] = score\n",
    "    print(f\"Recall@{k}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "pathlib.Path(\"figures\").mkdir(exist_ok=True)\n",
    "\n",
    "#draw with zoomed y-axis and finer ticks\n",
    "plt.figure(figsize=(5,3))\n",
    "bars = plt.bar([str(k) for k in ks], [results[k] for k in ks])\n",
    "\n",
    "#zoom into the 0.88-1 band\n",
    "plt.ylim(0.88, 1)\n",
    "# y-ticks every 0.02\n",
    "yt = np.arange(0.88, 1.01, 0.02)\n",
    "plt.yticks(yt, [f\"{y:.2f}\" for y in yt])\n",
    "\n",
    "plt.title(\"Baseline Recall@k\")\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"Recall\")\n",
    "plt.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "# save and show\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figures/recall_baseline_zoom.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## 6. Tweak Experiment - Embedding-Model Swap\n",
    "\n",
    "We leave the existing index intact and then build a second retriever with  \n",
    "`multi-qa-MiniLM-L6-dot-v1`. Finally, we recompute Recall@{1,3,5} and plot  \n",
    "baseline vs. multi-QA side by side. No changes to earlier cells required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "assert \"chunks\" in globals() and \"retriever_vs\" in globals(), \\\n",
    "    \"You need to run Section 3 (chunking & indexing) first\"\n",
    "\n",
    "from src.indexer import embed_and_index\n",
    "\n",
    "alt_retriever = embed_and_index(\n",
    "    chunks, \n",
    "    collection_name=\"ag_multiQA\", #new Chroma collection\n",
    "    persist_path=\"data/chroma\",\n",
    "    model_name=\"multi-qa-MiniLM-L6-dot-v1\" # the only change\n",
    ")\n",
    "\n",
    "print(\"Built alternative retriever with multi-qa-MiniLM-L6-dot-v1\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = [1, 3, 5]\n",
    "\n",
    "baseline_scores = {\n",
    "    k: recall_at_k(queries, retriever_vs.as_retriever(search_kwargs={\"k\":k}), k)\n",
    "    for k in ks\n",
    "}\n",
    "alt_scores = {\n",
    "    k: recall_at_k(queries, alt_retriever.as_retriever(search_kwargs={\"k\":k}), k)\n",
    "    for k in ks\n",
    "}\n",
    "\n",
    "print(\"Baseline recall:\", baseline_scores)\n",
    "print(\"Multi-QA Recall:\", alt_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "\n",
    "pathlib.Path(\"figures\").mkdir(exist_ok=True)\n",
    "\n",
    "x = np.arange(len(ks))\n",
    "width = 0.35\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.bar(x-width/2, [baseline_scores[k] for k in ks],\n",
    "        width, label=\"all-MiniLM-L6-v2\")\n",
    "plt.bar(x + width/2, [alt_scores[k] for k in ks],\n",
    "        width, label=\"multi-qa-MiniLM\")\n",
    "\n",
    "plt.xticks(x, [str(k) for k in ks])\n",
    "plt.ylim(0,1)\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"Recall\")\n",
    "plt.title(\"Recall@k: Baseline vs multi-qa-MiniLM\")\n",
    "plt.legend()\n",
    "plt.grid(axis=\"y\", alpha=0.3)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"figures/recall_comparison_embedding\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "## 7. Bonus Task - Matrix Benchmark of Embedding & Chunking\n",
    "We are using 2 different embedding models and 3 different chunking strategies to create a matrix benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import jsonlines, tqdm\n",
    "\n",
    "from src.chunker import chunk_documents\n",
    "from src.indexer import embed_and_index\n",
    "\n",
    "# load the golden queries (as dicts)\n",
    "with jsonlines.open(\"data/queries.jsonl\") as reader:\n",
    "    queries = list(reader)\n",
    "# each entry is now a dict with keys \"query\" and \"parent_id\"\n",
    "\n",
    "#re-define recall_at_k to match this structure\n",
    "def recall_at_k(queries, retriever, k):\n",
    "    \"\"\"\n",
    "    queries: list of dicts with keys \"query\" and \"parent_id\"\n",
    "    retriever: a LangChain retriever (with .invoke(...))\n",
    "    k: top-k cutoff\n",
    "    \"\"\"\n",
    "    hits = 0\n",
    "    for entry in queries:\n",
    "        q = entry[\"query\"]\n",
    "        gold = entry[\"parent_id\"]\n",
    "        docs = retriever.invoke(q)                             \n",
    "        top_parents = [d.metadata[\"parent_id\"] for d in docs]\n",
    "        if gold in top_parents:\n",
    "            hits += 1\n",
    "    return hits / len(queries)\n",
    "\n",
    "#set up our grid of embed-models × chunk-sizes\n",
    "embed_models  = [\"all-MiniLM-L6-v2\", \"multi-qa-MiniLM-L6-dot-v1\"]\n",
    "chunk_configs = [(400,50), (800,400), (64,16)]\n",
    "ks            = [1, 3, 5]\n",
    "chunk_labels  = [f\"{c}/{o}\" for c,o in chunk_configs]\n",
    "\n",
    "# init empty dataframe\n",
    "results = {\n",
    "    k: pd.DataFrame(index=embed_models, columns=chunk_labels, dtype=float)\n",
    "    for k in ks\n",
    "}\n",
    "\n",
    "#run every combo: re-chunk -> index -> eval\n",
    "for model_name, (cs, ov) in product(embed_models, chunk_configs):\n",
    "    label = f\"{cs}/{ov}\"\n",
    "    print(f\">> {model_name} | chunk={label} →\", end=\" \")\n",
    "\n",
    "    # re-chunk\n",
    "    alt_chunks = chunk_documents(docs, chunk_size=cs, chunk_overlap=ov)\n",
    "    print(f\"{len(alt_chunks)} chunks\")\n",
    "\n",
    "    #build fresh index under its own collection\n",
    "    retriever = embed_and_index(\n",
    "        alt_chunks,\n",
    "        collection_name=f\"{model_name.replace('/', '_')}_{cs}_{ov}\",\n",
    "        persist_path=\"data/chroma_grid\",\n",
    "        model_name=model_name\n",
    "    )\n",
    "\n",
    "    # compute recall@k for each k\n",
    "    for k in ks:\n",
    "        retr = retriever.as_retriever(search_kwargs={\"k\": k})\n",
    "        score = recall_at_k(queries, retr, k)\n",
    "        results[k].loc[model_name, label] = score\n",
    "        print(f\"Recall@{k}: {score:.3f}\")\n",
    "    print()\n",
    "\n",
    "for k, df in results.items():\n",
    "    mat = df.values.astype(float)\n",
    "\n",
    "    # heatmap\n",
    "    plt.figure(figsize=(5,3))\n",
    "    im = plt.imshow(mat, vmin=0, vmax=1, aspect=\"auto\")\n",
    "    plt.colorbar(im, label=\"Recall\")\n",
    "    plt.yticks(range(len(embed_models)), embed_models)\n",
    "    plt.xticks(range(len(chunk_labels)), chunk_labels, rotation=45)\n",
    "    plt.title(f\"Heatmap Recall@{k}\")\n",
    "    plt.xlabel(\"chunk_size/overlap\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"figures/recall_matrix_k{k}_heatmap.png\", dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "    # grouped bar chart\n",
    "    x = np.arange(len(embed_models))\n",
    "    width = 0.25\n",
    "    plt.figure(figsize=(6,3))\n",
    "    for i, lab in enumerate(chunk_labels):\n",
    "        scores = df[lab].astype(float).values\n",
    "        plt.bar(x + (i-1)*width, scores, width, label=lab)\n",
    "\n",
    "    plt.ylim(0,1)\n",
    "    plt.xticks(x, embed_models)\n",
    "    plt.ylabel(\"Recall\")\n",
    "    plt.title(f\"Recall@{k} by Embedding & Chunking\")\n",
    "    plt.legend(title=\"chunk/overlap\", bbox_to_anchor=(1.02,1), loc=\"upper left\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"figures/recall_matrix_k{k}_barchart.png\", dpi=150)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
